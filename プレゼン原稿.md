# タイトル

お世話になっております。今回は、インターンの西田　鴻が、
「Masked Entity Language Modelingによる固有表現抽出の制度改善の検討」として
発表を行います。よろしくお願いいたします。

15s

# P2 背景

この研究テーマを行う背景としまして、現状固有表現抽出には、未知語の抽出をするときの精度が低いという課題があることがあります。
例えば匿名化エンジンにおいて、、学習したデータとは異なる種類のデータにモデルを適用した際の精度が低いことであったり、
知見患者抽出において、学習したデータに含まれない単語、遺伝子検査を正しく認識できないことが具体例として挙げられます。

こうした課題、いわゆる汎化が足りない、という問題を解決する策の一つは学習データセットを充実させることだと思われますが、
特に医療用データセットは作成の際に医師など専門家の協力が必要になるなど、コストが非常に高くなるため、現実的ではありません。

そこで、アノテーションなしで、つまり専門家などの協力を必要としない方法で抽出精度を上げる事が重要となります。
こうした試みの中で、既存のデータセットを元に、語彙の操作やchatGPTなどを用いて人工的なデータを作成する「データ拡張」という手法があります。
データ拡張はすでに多数研究されていますが、そうした手法は主に英語のテキストを対象にしていたり、医療などとは異なる、一般的なドメインのテキストに適用している研究が多く、日本語の医療ドメインのテキストに対する研究例は少ないです。

1m37s

# P3 目的

そこで、本研究では、特に治験患者抽出における問題に焦点を当て、先行研究で提案されているデータ拡張の手法が、
医療系コーパスにおいても有効なのか調査を行いました。

数あるデータ拡張の手法の中でも、本研究では特に、Entity Maskingベースの手法について検討を行います。
これは、先行研究を調査する中で、Entity Maskingの考え方をベースにした手法が多く見られたためです。
今後の精度向上に向けた手法を考える際の有効性の検討の材料となることを目指しています。

45s

# P4 先行研究

Entity Masking手法の先行例としては、"Long-tail dataset entity recognition based on Data Augmentation"
を挙げます。
これは、訓練コーパスに含まれる分をランダムに選択し、含まれるエンティティを全て[MASK]というトークンに置き換えたものを合成データとする手法です。
作成した合成データは、元の訓練データと組み合わせ1つの拡張データセットとし、そのデータセットでNERを行います。
この手法では、特にデータセットの中で出現頻度が低いエンティティに対する検出精度が向上した事が報告されています。

本インターンでの研究テーマにおいては、この手法を再現し効果を確認しました。

1m3s

# P5 実験材料

実験に用いたデータセットについて説明します。
使用したデータセットは、iCorpus,MedTxt-CR-JA,Stockmarkの3つです。
iCorpus,MedTxtは医療ドメインのデータセットです。この二つはどちらもJ-STAGEで公開されている症例報告から作成されていますが、
iCorpusは症例報告の本文、MedTxtはアブストラクトから作成されているという違いがあります。
そのため、iCorpusはMedTxtよりも、症例あたりの文章量が長く、コンテキストを保持しているコーパスとなります。

Stockmarkは日本語Wikipediaから作成された、一般的な内容としてのコーパスです。

また、今回の実験において、iCorpusとMedTxtにおいては、精度評価の対象となるタグを絞っています。
具体的に、病名、臓器や体の名称、検査、薬品、処置　を精度評価の対象とします。

なお、Stockmarkに関しては、アノテーションされているすべてのタグを評価対象とします。

# P6 実験材料の例

コーパスに含まれるテキストのサンプルを示します。ここでは、実際のコーパスの文章の傾向を確認いただければと思います。

# P8 実験設定

次に実験設定の説明です。使用するモデルは、東北大が作成したBert-Base-JapaneseのV3を採用しました。
このモデルは、CC-100データセット、英語版のwikipediaなどのデータセットですね、これを学習したBert-Baseを元に、さらに日本語のWikipediaで継続して学習を行ったモデルです。
このモデルに、線形層を追加して、トークン分類、NERを行っています。
エポック数は100、最大入力トークンは、iCorpusが512,MedTxt-CR-JAが256、Stockmarkが128となっています。
これらは、各コーパスの1データのテキストが95%以上入力できる長さで設定をしました。
バッチサイズは訓練で32,検証で256、最適化関数はAdamWです。
各データセットの分割は、まず学習用とテスト用で8:2に分割し、学習用データセットを、Training用と検証用に9:1で分割をしています。また、分割は症例ごとに行いました。

1m14s

# P.10 実験方法

次に、実験を行った手法について説明します。基本的な方法は先行研究で示したものと同じですが、
文章をランダムに選択し、その文に含まれるEntityを[MASK]というトークンに置き換え、置き換えたデータを合成データとします。
作成した合成データを、元のコーパスと組み合わせて拡張データセットとします。
この拡張データを用いてNERを学習し、ベースラインからの精度変化を比較します。

37s

# P.11 実験方法(2)

ただし、iCorpusとMedTxtはエンティティの密度が高く、すべてのエンティティをマスクすると文脈の喪失が大きすぎるため、
エンティティのマスクを確率で行うこととします。確率のパラメータは、100%、60%、40%としました。

23s

# P.12 実験方法(2) マスキングの2つの戦略

また、マスキングの際に異なる2つの方法で実験を行っています。
一つ目は Whole Entity Replacementと定義しています。これは、マスキングの前にEntityをトークナイズし、トークナイズした結果の各トークンをそれぞれマスキングします。
図においては左側ですが、仮に「糖尿病」という単語をトークナイズした結果「糖尿」と「病」というトークンに分解される場合、これら2つのトークンが[MASK]トークンに置き換えられ、結果として二つの[MASK]トークンに置換されることになります。
一方、二つ目は、Replacementと定義していますが、これはEntityのトークン化などは行わなず、一つのEntityは一つの[MASK]と置き換えます。図においては右側です。先行研究で挙げた研究ではこちらの方法でした。

1m16s

# P.13 評価方法

精度の評価方法ですが、既知後と未知語の検出精度としました。
既知語と未知語の定義は、Entityのうち、テスト用データセットと学習用データセット両方に含まれるEntityを既知語、
テスト用データセットには含まれるが学習用データセットには含まれないEntityを未知語としています。

