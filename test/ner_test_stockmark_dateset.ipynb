{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f89fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishida/projects/INTRN/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34884498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのロード\n",
    "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
    "\n",
    "# 固有表現のタイプとIDを対応付る辞書 \n",
    "type_id_dict = {\n",
    "    \"人名\": 1,\n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d41774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
    "    for e in sample[\"entities\"]:\n",
    "        e['type_id'] = type_id_dict[e['type']]\n",
    "        del e['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62918097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 3205\n",
      "Length of val: 1068\n",
      "Length of test: 1070\n"
     ]
    }
   ],
   "source": [
    "# データセットの分割\n",
    "n = len(dataset)\n",
    "n_train = int(n*0.6)\n",
    "n_val = int(n*0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:n_train+n_val]\n",
    "dataset_test = dataset[n_train+n_val:]\n",
    "\n",
    "print(f\"Length of train: {len(dataset_train)}\")\n",
    "print(f\"Length of val: {len(dataset_val)}\")\n",
    "print(f\"Length of test: {len(dataset_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa72d52",
   "metadata": {},
   "source": [
    "# トークナイザーの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8197c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerTokenizerForTrain(BertJapaneseTokenizer):\n",
    "\n",
    "  def create_tokens_and_labels(self, splitted):\n",
    "      \"\"\"分割された文字列をトークン化し、ラベルを付与\n",
    "      Args：\n",
    "        splitted: 分割された文字列\n",
    "          例：\n",
    "          [{'text': 'レッドフォックス株式会社', 'label': 2},\n",
    "          {'text': 'は、', 'label': 0},\n",
    "          {'text': '東京都千代田区', 'label': 5},\n",
    "          {'text': 'に本社を置くITサービス企業である。', 'label': 0}]\n",
    "      Return:\n",
    "        tokens, labels\n",
    "          例：\n",
    "          ['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
    "          [2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      \"\"\"\n",
    "      tokens = [] # トークン格納用\n",
    "      labels = [] # トークンに対応するラベル格納用\n",
    "      for s in splitted:\n",
    "          text = s['text']\n",
    "          label = s['label']\n",
    "          tokens_splitted = self.tokenize(text) # BertJapaneseTokenizerのトークナイザを使ってトークンに分割\n",
    "          labels_splitted = [label] * len(tokens_splitted)\n",
    "          tokens.extend(tokens_splitted)\n",
    "          labels.extend(labels_splitted)\n",
    "      \n",
    "      return tokens, labels\n",
    "\n",
    "\n",
    "  def encoding_for_bert(self, tokens, labels, max_length):\n",
    "      \"\"\"符号化を行いBERTに入力できる形式にする\n",
    "      Args:\n",
    "        tokens: トークン列\n",
    "        labels: トークンに対応するラベルの列\n",
    "      Returns: \n",
    "        encoding: BERTに入力できる形式\n",
    "        例：\n",
    "        {'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "        　'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "        　'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "          'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "\n",
    "      \"\"\"\n",
    "      encoding = self.encode_plus(\n",
    "          tokens, \n",
    "          max_length=max_length, \n",
    "          padding='max_length', \n",
    "          truncation=True\n",
    "      ) \n",
    "      # トークン[CLS]、[SEP]のラベルを0\n",
    "      # [SEP]は文の区切りを示すトークン\n",
    "      labels = [0] + labels[:max_length-2] + [0] # リストの最初と最後にラベルとして0を追加。CLSとSEPが増えた分の2トークンを加味して、最終的に\n",
    "      # max_lengthの長さ分でラベルを切り詰める。(truncationにより、max長を超えたトークンは削除されている。それに合わせる・)\n",
    "      labels = labels + [0]*( max_length - len(labels) ) # max長までpaddingトークンの分、ラベルに0を敷き詰める。\n",
    "      encoding['labels'] = labels\n",
    "\n",
    "      return encoding\n",
    "\n",
    "\n",
    "  def encode_plus_tagged(self, text, entities, max_length):\n",
    "      \"\"\"文章とそれに含まれる固有表現が与えられた時に、符号化とラベル列の作成\n",
    "      Args:\n",
    "        text: 元の文章\n",
    "        entities: 文章中の固有表現の位置(span)とラベル(type_id)の情報\n",
    "\n",
    "      \"\"\"\n",
    "      # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
    "      entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現のスタート位置を用いて、昇順でソート\n",
    "      \"\"\"lambdaは、使い捨ての関数のようなもの。気になるならしらべる。\"\"\"\n",
    "      splitted = [] # 分割後の文字列格納用\n",
    "      position = 0\n",
    "      for entity in entities: \n",
    "          start = entity['span'][0]\n",
    "          end = entity['span'][1]\n",
    "          label = entity['type_id']\n",
    "          # 固有表現ではないものには0のラベルを付与\n",
    "          splitted.append({'text': text[position:start], 'label':0}) \n",
    "          \"\"\"\n",
    "          前回のループでカウンターは直近の固有表現の次の文字のところに来ている。ここから、次のエンティティのstart位置\n",
    "          までは固有表現ではないので、0を付与する。\n",
    "          \"\"\"  \n",
    "          # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
    "          splitted.append({'text': text[start:end], 'label':label}) \n",
    "          position = end\n",
    "\n",
    "          \"\"\"\n",
    "          ここまでで出来上がるのは以下のようなもの。\n",
    "          splitted = [\n",
    "              {'text': '東京大学', 'label': 3},      # 組織名\n",
    "              {'text': 'の', 'label': 0},           # 非固有表現\n",
    "              {'text': '田中教授', 'label': 1},      # 人名\n",
    "              {'text': 'が研究を発表した。', 'label': 0}  # 非固有表現\n",
    "          ]\n",
    "          \"\"\"\n",
    "      # 最後の固有表現から文末に、0のラベルを付与\n",
    "      splitted.append({'text': text[position:], 'label':0})\n",
    "      # positionとspan[0]の値が同じだと空白文字にラベル0が付与されるため、長さ0の文字列は除く（例：{'text': '', 'label': 0}）\n",
    "      splitted = [ s for s in splitted if s['text'] ] \n",
    "\n",
    "      \"\"\"\n",
    "      ここの処理について\n",
    "\n",
    "      span[0]は固有表現のスタート文字位置。上のコードでは、固有表現へのタグ付が終わったらその後には非固有表現が続くと想定されている。\n",
    "      そのため、固有表現のラベル格納がついた後のポジションから次の固有表現のスタート1文字手前まで0を与えることになっているが、\n",
    "      固有表現のすぐ後に別の固有表現が続くなどの場合で、positionがすでに次の固有表現のスタートにいる場合は空白文字に0が埋め込まれる。\n",
    "      そのため、 if s['text] がtrueになるもののみを格納することになっている\n",
    "      \"\"\"\n",
    "\n",
    "      # 分割された文字列をトークン化し、ラベルを付与\n",
    "      tokens, labels = self.create_tokens_and_labels(splitted)\n",
    "\n",
    "      # 符号化を行いBERTに入力できる形式にする\n",
    "      encoding = self.encoding_for_bert(tokens, labels, max_length)\n",
    "\n",
    "      return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9715cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NerTokenizerForTrain'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---pprint.pprint(tmp)の実行---\n",
      "{'curid': '2415078',\n",
      " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
      "              {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}],\n",
      " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。'}\n",
      "---tokenizer.encode_plus_tagged---の実行\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'input_ids': [2, 3990, 13779, 1114, 811, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'labels': [0, 2, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NerTokenizerForTrain.from_pretrained(MODEL_NAME)\n",
    "import pprint\n",
    "tmp = dataset_train[1]\n",
    "print(\"---pprint.pprint(tmp)の実行---\")\n",
    "pprint.pprint(tmp)\n",
    "print(\"---tokenizer.encode_plus_tagged---の実行\")\n",
    "pprint.pprint(tokenizer.encode_plus_tagged(text=tmp[\"text\"], entities=tmp[\"entities\"], max_length=32), width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a126c7",
   "metadata": {},
   "source": [
    "データセットのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30cd239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "  \"\"\"データセット作成\n",
    "  \"\"\"\n",
    "  def __init__(self, dataset, tokenizer, max_length): \n",
    "    \"\"\"__(アンダーバー2つ)で囲っているのは、特殊メソッドというもの。名前と呼び出されるタイミングが\n",
    "    決まっており、その中身（何をするか）はこちらで指定する。\n",
    "    \"\"\"\n",
    "    self.dataset = dataset\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    text = self.dataset[index][\"text\"]\n",
    "    entities = self.dataset[index][\"entities\"]\n",
    "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=self.max_length) #さっき書いたやつ\n",
    "\n",
    "    input_ids = torch.tensor(encoding[\"input_ids\"])\n",
    "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
    "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
    "    labels = torch.tensor(encoding[\"labels\"]) #tokenizer.encode_plus_taggedで取得した情報をそれぞれに格納する。\n",
    "\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_mask,\n",
    "      \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022ab7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "dataset_train_for_loader = CreateDataset(dataset_train, tokenizer, max_length=128)\n",
    "dataset_val_for_loader = CreateDataset(dataset_val, tokenizer, max_length=128)\n",
    "\n",
    "# データローダーの作成\n",
    "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True, pin_memory=True)\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256, shuffle=True, pin_memory=True)\n",
    "\n",
    "dataloaders_dict = {\"train\": dataloader_train, \"val\": dataloader_val}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d802d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU使えるならGPU使う\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 学習済みモデルのロード\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n",
    "\n",
    "# モデルをGPUへ転送\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9113c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化器\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3422f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            iteration = 1\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BERTに入力\n",
    "                    loss, logits = model(input_ids=input_ids, \n",
    "                                          token_type_ids=None, \n",
    "                                          attention_mask=attention_mask, \n",
    "                                          labels=labels,\n",
    "                                          return_dict=False)\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            print(f\"イテレーション {iteration} || Loss: {loss:.4f}\")\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "\n",
    "            # epochごとのloss\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | phase {phase} |  Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba95875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "イテレーション 10 || Loss: 0.4466\n",
      "イテレーション 20 || Loss: 0.2510\n",
      "イテレーション 30 || Loss: 0.1879\n",
      "イテレーション 40 || Loss: 0.1124\n",
      "イテレーション 50 || Loss: 0.1130\n",
      "イテレーション 60 || Loss: 0.0773\n",
      "イテレーション 70 || Loss: 0.0419\n",
      "イテレーション 80 || Loss: 0.0648\n",
      "イテレーション 90 || Loss: 0.0354\n",
      "イテレーション 100 || Loss: 0.0397\n",
      "Epoch 1/3 | phase train |  Loss: 0.2407\n",
      "Epoch 1/3 | phase val |  Loss: 0.0048\n",
      "イテレーション 10 || Loss: 0.0350\n",
      "イテレーション 20 || Loss: 0.0204\n",
      "イテレーション 30 || Loss: 0.0268\n",
      "イテレーション 40 || Loss: 0.0223\n",
      "イテレーション 50 || Loss: 0.0270\n",
      "イテレーション 60 || Loss: 0.0182\n",
      "イテレーション 70 || Loss: 0.0186\n",
      "イテレーション 80 || Loss: 0.0250\n",
      "イテレーション 90 || Loss: 0.0163\n",
      "イテレーション 100 || Loss: 0.0212\n",
      "Epoch 2/3 | phase train |  Loss: 0.0275\n",
      "Epoch 2/3 | phase val |  Loss: 0.0041\n",
      "イテレーション 10 || Loss: 0.0174\n",
      "イテレーション 20 || Loss: 0.0223\n",
      "イテレーション 30 || Loss: 0.0226\n",
      "イテレーション 40 || Loss: 0.0090\n",
      "イテレーション 50 || Loss: 0.0173\n",
      "イテレーション 60 || Loss: 0.0100\n",
      "イテレーション 70 || Loss: 0.0103\n",
      "イテレーション 80 || Loss: 0.0174\n",
      "イテレーション 90 || Loss: 0.0251\n",
      "イテレーション 100 || Loss: 0.0117\n",
      "Epoch 3/3 | phase train |  Loss: 0.0137\n",
      "Epoch 3/3 | phase val |  Loss: 0.0045\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行\n",
    "num_epochs = 3\n",
    "net_trained = train_model(model, dataloaders_dict, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# 学習済みモデルを保存\n",
    "torch.save(net_trained.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fb3f5",
   "metadata": {},
   "source": [
    "# :固有表現の推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerTokenizerForTest(BertJapaneseTokenizer):\n",
    "\n",
    "    def encoding_for_bert(self, tokens, max_length):\n",
    "        \"\"\"符号化を行いBERTに入力できる形式にする\n",
    "        Args:\n",
    "          tokens: トークン列\n",
    "        Returns: \n",
    "          encoding: BERTに入力できる形式\n",
    "          例：\n",
    "          {'input_ids': [2, 106, 6, 946, 674, 5, 12470, 9921, 5, 859, 6, 2446, 22903, 35, 24831, 11614, 35, 2176, 2200, 35, 3700, 29650, 2446, 333, 9, 6, 2409, 109, 5, 333, 3849, 3], \n",
    "          'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "          'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "        \"\"\"\n",
    "        encoding = self.encode_plus(\n",
    "            tokens, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_tensors = \"pt\"\n",
    "        ) \n",
    "\n",
    "        return encoding\n",
    "\n",
    "\n",
    "    def create_spans_of_token(self, tokens_original, encoding):\n",
    "        \"\"\" 各トークン（サブワード）の文章中での位置を調べる\n",
    "          Args:\n",
    "            tokens_original: 固有表現単位で分割されたものではなく、BERTのトークナイザーで分割したトークン列。\n",
    "            encoding: \n",
    "            例：tokens_original\n",
    "              ['元々', 'は', '前作', '「', 'The', 'Apple', 's', '」', 'の', 'アウト', ...]\n",
    "          \n",
    "          Return:\n",
    "            spans: 各トークンの文章中の位置([CLS][PAD]などの特殊トークンはダミーで置き換える)\n",
    "            例：\n",
    "              [[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 9], [10, 15], [15, 16], ...]\n",
    "        \"\"\"        \n",
    "        position = 0\n",
    "        spans = [] # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position:position+l]: # !=で、空白である場合Falseになり、if文の中身が実行される。\n",
    "                    \"\"\"例：英語文章のように空白が混ざっていると下記のようにずれるケースがあることを考慮\n",
    "                          token: \"Digital\"\n",
    "                          text[position:position+l]: \" Digita\" # 先頭の空白分の文字数が発生している\n",
    "                    \"\"\"\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position+l]) # カウンタの位置から、トークンの文字列分進めたもの。\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        sequence_length = len(encoding['input_ids'])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
    "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) )  # [-1,-1]は、「文章のどこにも対応しない位置」ということで、探すな、という意味。\n",
    "\n",
    "        return spans\n",
    "\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None):\n",
    "        \"\"\"文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        \"\"\"\n",
    "        Mecab での分析を利用しているのを初めてみた。これはNERだから特に行っているみたい。\n",
    "        Mecab はBertJapaneseTokenizerに一緒に含まれているらしい。\n",
    "        \"\"\"\n",
    "        tokens = [] # トークン格納用\n",
    "        tokens_original = [] # トークンに対応する文章中の文字列格納用\n",
    "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend([\n",
    "                    token.replace('##','') for token in tokens_word\n",
    "                ])\n",
    "\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする\n",
    "        encoding = self.encoding_for_bert(tokens, max_length)\n",
    "\n",
    "        # 各トークン（サブワード）の文章中での位置を調べる\n",
    "        spans = self.create_spans_of_token(tokens_original, encoding)\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "        \"\"\"\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
    "        entities = []\n",
    "        position = 0\n",
    "        for label, group in itertools.groupby(labels):\n",
    "            \"\"\"\n",
    "            例：labelsは予測結果\n",
    "            labels: [0, 0, 0, 3, 3, 5, 7, 7, 7, 0, 0, 0]\n",
    "\n",
    "            `itertools.groupby`について:連続する同じ値をグループ化する関数\n",
    "            \"\"\"\n",
    "            start_idx = position # 連続するラベルの先頭位置\n",
    "            end_idx = position + len(list(group)) - 1 # 連続するラベルの最終位置\n",
    "            \n",
    "            # (encode_plus_untaggedで計算した)spansから、文章中の位置を特定\n",
    "            start = spans[start_idx][0] \n",
    "            end = spans[end_idx][1]\n",
    "            \n",
    "            # 次のspanの位置に更新\n",
    "            position = end_idx + 1\n",
    "\n",
    "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],\n",
    "                    \"span\": [start, end],\n",
    "                    \"type_id\": label\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c6a8a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NerTokenizerForTest'.\n"
     ]
    }
   ],
   "source": [
    "# テスト時に使うトークナイザーをロード\n",
    "tokenizer = NerTokenizerForTest.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57abe97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1070/1070 [00:05<00:00, 184.25it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(text, tokenizer, model):\n",
    "    \"\"\"BERTで固有表現抽出を行うための関数。\n",
    "    \"\"\"\n",
    "    # 符号化\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(text)\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "    # ラベルの予測値の計算\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        scores = output.logits\n",
    "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
    "\n",
    "    # ラベル列を固有表現に変換\n",
    "    entities = tokenizer.convert_bert_output_to_entities(\n",
    "        text, labels_predicted, spans\n",
    "    )\n",
    "\n",
    "    return entities\n",
    "\n",
    "# 固有表現抽出\n",
    "entities_list = [] # 正解の固有表現\n",
    "entities_predicted_list = [] # 予測された固有表現\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample['text']\n",
    "    entities_predicted = predict(text, tokenizer, net_trained) # BERTで予測\n",
    "    entities_list.append(sample['entities'])\n",
    "    entities_predicted_list.append( entities_predicted )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cba79",
   "metadata": {},
   "source": [
    "保存済みのモデルを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399dfc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "# 1. 新しいモデルのインスタンスを作成\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_loaded = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n",
    "\n",
    "# 2. 保存した重みを読み込む\n",
    "model_loaded.load_state_dict(torch.load('model_weights.pth'))\n",
    "net_trained = model_loaded\n",
    "\n",
    "# 3. モデルを評価モードにする & 4. GPUに転送する\n",
    "net_trained.eval()\n",
    "net_trained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ccf934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1070/1070 [00:05<00:00, 181.12it/s]\n"
     ]
    }
   ],
   "source": [
    "entities_list = [] # 正解の固有表現\n",
    "entities_predicted_list = [] # 予測された固有表現\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample['text']\n",
    "    entities_predicted = predict(text, tokenizer, net_trained) # BERTで予測\n",
    "    entities_list.append(sample['entities'])\n",
    "    entities_predicted_list.append( entities_predicted )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0720b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 正解 #\n",
      "[{'name': 'グリーンウッド', 'span': [12, 19], 'type_id': 1}, {'name': 'ジョン・ライアル', 'span': [36, 44], 'type_id': 1}]\n",
      "# 推論 #\n",
      "[{'name': 'グリーンウッド', 'span': [12, 19], 'type_id': 1}, {'name': 'ジョン・ライアル', 'span': [36, 44], 'type_id': 1}]\n",
      "# もとの文章 #\n",
      "1974-75シーズンにグリーンウッドは総合監督に就任し、アシスタントのジョン・ライアルを監督に任命した。\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "print(\"# 正解 #\")\n",
    "print(entities_list[i])\n",
    "print(\"# 推論 #\")\n",
    "print(entities_predicted_list[i])\n",
    "print(\"# もとの文章 #\")\n",
    "print(dataset_test[i][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
